{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76d0d9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                    Linear Vision Transformer (ViT) Summary                     \n",
      "================================================================================\n",
      "Image size: 224x224\n",
      "Patch size: 16x16\n",
      "Input channels: 3\n",
      "Embedding dimension: 768\n",
      "Number of heads: 8\n",
      "Number of layers: 8\n",
      "MLP ratio: 4.0\n",
      "Attention reduction ratio: 4\n",
      "Number of classes: 2\n",
      "================================================================================\n",
      "\n",
      "Layer Details:\n",
      "\n",
      "Input                    | Shape: (batch_size, 3, 224, 224) | Params: 0         | Details: Input image\n",
      "PatchEmbedding           | Shape: (batch_size, 196, 768)    | Params: 590,592   | Details: img_size=224, patch_size=16, in_channels=3, embed_dim=768\n",
      "PositionEmbedding        | Shape: (1, 197, 768)             | Params: 152,064   | Details: Learned positional embeddings + class token\n",
      "PosDrop                  | Shape: (batch_size, 197, 768)    | Params: 0         | Details: Dropout(p=0.1)\n",
      "LinearTransformerBlock_1 | Shape: (batch_size, 197, 768)    | Params: 5,316,672 | Details: dim=768, num_heads=8, mlp_ratio=4.0, reduction_ratio=4\n",
      "LinearTransformerBlock_2 | Shape: (batch_size, 197, 768)    | Params: 5,316,672 | Details: dim=768, num_heads=8, mlp_ratio=4.0, reduction_ratio=4\n",
      "LinearTransformerBlock_3 | Shape: (batch_size, 197, 768)    | Params: 5,316,672 | Details: dim=768, num_heads=8, mlp_ratio=4.0, reduction_ratio=4\n",
      "LinearTransformerBlock_4 | Shape: (batch_size, 197, 768)    | Params: 5,316,672 | Details: dim=768, num_heads=8, mlp_ratio=4.0, reduction_ratio=4\n",
      "LinearTransformerBlock_5 | Shape: (batch_size, 197, 768)    | Params: 5,316,672 | Details: dim=768, num_heads=8, mlp_ratio=4.0, reduction_ratio=4\n",
      "LinearTransformerBlock_6 | Shape: (batch_size, 197, 768)    | Params: 5,316,672 | Details: dim=768, num_heads=8, mlp_ratio=4.0, reduction_ratio=4\n",
      "LinearTransformerBlock_7 | Shape: (batch_size, 197, 768)    | Params: 5,316,672 | Details: dim=768, num_heads=8, mlp_ratio=4.0, reduction_ratio=4\n",
      "LinearTransformerBlock_8 | Shape: (batch_size, 197, 768)    | Params: 5,316,672 | Details: dim=768, num_heads=8, mlp_ratio=4.0, reduction_ratio=4\n",
      "LayerNorm                | Shape: (batch_size, 197, 768)    | Params: 1,536     | Details: LayerNorm(768)\n",
      "Head                     | Shape: (batch_size, 2)           | Params: 1,538     | Details: Linear(768->2)\n",
      "\n",
      "================================================================================\n",
      "Total Parameters         : 43,279,106\n",
      "================================================================================\n",
      "\n",
      "Standard Torch Summary:\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
      "    PatchEmbedding-2             [-1, 196, 768]               0\n",
      "           Dropout-3             [-1, 197, 768]               0\n",
      "         LayerNorm-4             [-1, 197, 768]           1,536\n",
      "            Linear-5             [-1, 197, 192]         147,648\n",
      "            Linear-6             [-1, 197, 384]         295,296\n",
      "           Dropout-7          [-1, 8, 197, 197]               0\n",
      "            Linear-8             [-1, 197, 768]         148,224\n",
      "           Dropout-9             [-1, 197, 768]               0\n",
      "ReducedLinearAttention-10             [-1, 197, 768]               0\n",
      "        LayerNorm-11             [-1, 197, 768]           1,536\n",
      "           Linear-12            [-1, 197, 3072]       2,362,368\n",
      "             GELU-13            [-1, 197, 3072]               0\n",
      "          Dropout-14            [-1, 197, 3072]               0\n",
      "           Linear-15             [-1, 197, 768]       2,360,064\n",
      "          Dropout-16             [-1, 197, 768]               0\n",
      "              MLP-17             [-1, 197, 768]               0\n",
      "LinearTransformerBlock-18             [-1, 197, 768]               0\n",
      "        LayerNorm-19             [-1, 197, 768]           1,536\n",
      "           Linear-20             [-1, 197, 192]         147,648\n",
      "           Linear-21             [-1, 197, 384]         295,296\n",
      "          Dropout-22          [-1, 8, 197, 197]               0\n",
      "           Linear-23             [-1, 197, 768]         148,224\n",
      "          Dropout-24             [-1, 197, 768]               0\n",
      "ReducedLinearAttention-25             [-1, 197, 768]               0\n",
      "        LayerNorm-26             [-1, 197, 768]           1,536\n",
      "           Linear-27            [-1, 197, 3072]       2,362,368\n",
      "             GELU-28            [-1, 197, 3072]               0\n",
      "          Dropout-29            [-1, 197, 3072]               0\n",
      "           Linear-30             [-1, 197, 768]       2,360,064\n",
      "          Dropout-31             [-1, 197, 768]               0\n",
      "              MLP-32             [-1, 197, 768]               0\n",
      "LinearTransformerBlock-33             [-1, 197, 768]               0\n",
      "        LayerNorm-34             [-1, 197, 768]           1,536\n",
      "           Linear-35             [-1, 197, 192]         147,648\n",
      "           Linear-36             [-1, 197, 384]         295,296\n",
      "          Dropout-37          [-1, 8, 197, 197]               0\n",
      "           Linear-38             [-1, 197, 768]         148,224\n",
      "          Dropout-39             [-1, 197, 768]               0\n",
      "ReducedLinearAttention-40             [-1, 197, 768]               0\n",
      "        LayerNorm-41             [-1, 197, 768]           1,536\n",
      "           Linear-42            [-1, 197, 3072]       2,362,368\n",
      "             GELU-43            [-1, 197, 3072]               0\n",
      "          Dropout-44            [-1, 197, 3072]               0\n",
      "           Linear-45             [-1, 197, 768]       2,360,064\n",
      "          Dropout-46             [-1, 197, 768]               0\n",
      "              MLP-47             [-1, 197, 768]               0\n",
      "LinearTransformerBlock-48             [-1, 197, 768]               0\n",
      "        LayerNorm-49             [-1, 197, 768]           1,536\n",
      "           Linear-50             [-1, 197, 192]         147,648\n",
      "           Linear-51             [-1, 197, 384]         295,296\n",
      "          Dropout-52          [-1, 8, 197, 197]               0\n",
      "           Linear-53             [-1, 197, 768]         148,224\n",
      "          Dropout-54             [-1, 197, 768]               0\n",
      "ReducedLinearAttention-55             [-1, 197, 768]               0\n",
      "        LayerNorm-56             [-1, 197, 768]           1,536\n",
      "           Linear-57            [-1, 197, 3072]       2,362,368\n",
      "             GELU-58            [-1, 197, 3072]               0\n",
      "          Dropout-59            [-1, 197, 3072]               0\n",
      "           Linear-60             [-1, 197, 768]       2,360,064\n",
      "          Dropout-61             [-1, 197, 768]               0\n",
      "              MLP-62             [-1, 197, 768]               0\n",
      "LinearTransformerBlock-63             [-1, 197, 768]               0\n",
      "        LayerNorm-64             [-1, 197, 768]           1,536\n",
      "           Linear-65             [-1, 197, 192]         147,648\n",
      "           Linear-66             [-1, 197, 384]         295,296\n",
      "          Dropout-67          [-1, 8, 197, 197]               0\n",
      "           Linear-68             [-1, 197, 768]         148,224\n",
      "          Dropout-69             [-1, 197, 768]               0\n",
      "ReducedLinearAttention-70             [-1, 197, 768]               0\n",
      "        LayerNorm-71             [-1, 197, 768]           1,536\n",
      "           Linear-72            [-1, 197, 3072]       2,362,368\n",
      "             GELU-73            [-1, 197, 3072]               0\n",
      "          Dropout-74            [-1, 197, 3072]               0\n",
      "           Linear-75             [-1, 197, 768]       2,360,064\n",
      "          Dropout-76             [-1, 197, 768]               0\n",
      "              MLP-77             [-1, 197, 768]               0\n",
      "LinearTransformerBlock-78             [-1, 197, 768]               0\n",
      "        LayerNorm-79             [-1, 197, 768]           1,536\n",
      "           Linear-80             [-1, 197, 192]         147,648\n",
      "           Linear-81             [-1, 197, 384]         295,296\n",
      "          Dropout-82          [-1, 8, 197, 197]               0\n",
      "           Linear-83             [-1, 197, 768]         148,224\n",
      "          Dropout-84             [-1, 197, 768]               0\n",
      "ReducedLinearAttention-85             [-1, 197, 768]               0\n",
      "        LayerNorm-86             [-1, 197, 768]           1,536\n",
      "           Linear-87            [-1, 197, 3072]       2,362,368\n",
      "             GELU-88            [-1, 197, 3072]               0\n",
      "          Dropout-89            [-1, 197, 3072]               0\n",
      "           Linear-90             [-1, 197, 768]       2,360,064\n",
      "          Dropout-91             [-1, 197, 768]               0\n",
      "              MLP-92             [-1, 197, 768]               0\n",
      "LinearTransformerBlock-93             [-1, 197, 768]               0\n",
      "        LayerNorm-94             [-1, 197, 768]           1,536\n",
      "           Linear-95             [-1, 197, 192]         147,648\n",
      "           Linear-96             [-1, 197, 384]         295,296\n",
      "          Dropout-97          [-1, 8, 197, 197]               0\n",
      "           Linear-98             [-1, 197, 768]         148,224\n",
      "          Dropout-99             [-1, 197, 768]               0\n",
      "ReducedLinearAttention-100             [-1, 197, 768]               0\n",
      "       LayerNorm-101             [-1, 197, 768]           1,536\n",
      "          Linear-102            [-1, 197, 3072]       2,362,368\n",
      "            GELU-103            [-1, 197, 3072]               0\n",
      "         Dropout-104            [-1, 197, 3072]               0\n",
      "          Linear-105             [-1, 197, 768]       2,360,064\n",
      "         Dropout-106             [-1, 197, 768]               0\n",
      "             MLP-107             [-1, 197, 768]               0\n",
      "LinearTransformerBlock-108             [-1, 197, 768]               0\n",
      "       LayerNorm-109             [-1, 197, 768]           1,536\n",
      "          Linear-110             [-1, 197, 192]         147,648\n",
      "          Linear-111             [-1, 197, 384]         295,296\n",
      "         Dropout-112          [-1, 8, 197, 197]               0\n",
      "          Linear-113             [-1, 197, 768]         148,224\n",
      "         Dropout-114             [-1, 197, 768]               0\n",
      "ReducedLinearAttention-115             [-1, 197, 768]               0\n",
      "       LayerNorm-116             [-1, 197, 768]           1,536\n",
      "          Linear-117            [-1, 197, 3072]       2,362,368\n",
      "            GELU-118            [-1, 197, 3072]               0\n",
      "         Dropout-119            [-1, 197, 3072]               0\n",
      "          Linear-120             [-1, 197, 768]       2,360,064\n",
      "         Dropout-121             [-1, 197, 768]               0\n",
      "             MLP-122             [-1, 197, 768]               0\n",
      "LinearTransformerBlock-123             [-1, 197, 768]               0\n",
      "       LayerNorm-124             [-1, 197, 768]           1,536\n",
      "          Linear-125                    [-1, 2]           1,538\n",
      "================================================================\n",
      "Total params: 43,127,042\n",
      "Trainable params: 43,127,042\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 224.40\n",
      "Params size (MB): 164.52\n",
      "Estimated Total Size (MB): 389.49\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from math import sqrt\n",
    "from collections import OrderedDict\n",
    "from torchsummary import summary\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.proj(x) \n",
    "        x = x.flatten(2)  \n",
    "        x = x.transpose(1, 2)  \n",
    "        return x\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return (f\"img_size={self.img_size}, patch_size={self.patch_size}, \"\n",
    "                f\"in_channels={self.proj.in_channels}, embed_dim={self.proj.out_channels}\")\n",
    "\n",
    "class ReducedLinearAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0., reduction_ratio=2):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.reduced_dim = dim // reduction_ratio\n",
    "        self.head_dim = self.reduced_dim // num_heads\n",
    "        self.scale = 1.0 / sqrt(self.head_dim)\n",
    "        \n",
    "        self.q = nn.Linear(dim, self.reduced_dim, bias=qkv_bias)\n",
    "        self.kv = nn.Linear(dim, self.reduced_dim * 2, bias=qkv_bias)  \n",
    "        \n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(self.reduced_dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        # Project queries\n",
    "        q = self.q(x).reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        # Project shared keys and values\n",
    "        kv = self.kv(x).reshape(B, N, 2, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        k, v = kv[0], kv[1]  \n",
    "        \n",
    "        # Linear attention with exponential kernel\n",
    "        k = k * self.scale\n",
    "        attn = torch.exp(q @ k.transpose(-2, -1))\n",
    "        attn = attn / attn.sum(dim=-1, keepdim=True)\n",
    "        attn = self.attn_drop(attn)\n",
    "        \n",
    "        x = torch.matmul(attn, v) \n",
    "        x = x.transpose(1, 2).reshape(B, N, self.reduced_dim)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return (f\"dim={self.dim}, num_heads={self.num_heads}, reduced_dim={self.reduced_dim}, \"\n",
    "                f\"head_dim={self.head_dim}, reduction_ratio={self.dim//self.reduced_dim}\")\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features * 4\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return (f\"in_features={self.fc1.in_features}, \"\n",
    "                f\"hidden_features={self.fc1.out_features}, \"\n",
    "                f\"out_features={self.fc2.out_features}\")\n",
    "\n",
    "class LinearTransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0., reduction_ratio=2):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = ReducedLinearAttention(\n",
    "            dim, \n",
    "            num_heads=num_heads, \n",
    "            qkv_bias=qkv_bias, \n",
    "            attn_drop=attn_drop, \n",
    "            proj_drop=drop,\n",
    "            reduction_ratio=reduction_ratio\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = MLP(\n",
    "            in_features=dim,\n",
    "            hidden_features=int(dim * mlp_ratio),\n",
    "            drop=drop\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return (f\"dim={self.attn.dim}, num_heads={self.attn.num_heads}, \"\n",
    "                f\"mlp_ratio={self.mlp.fc1.out_features/self.attn.dim:.1f}, \"\n",
    "                f\"reduction_ratio={self.attn.dim//self.attn.reduced_dim}\")\n",
    "\n",
    "class LinearViT(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=224,\n",
    "        patch_size=16,\n",
    "        in_channels=3,\n",
    "        num_classes=1000,\n",
    "        embed_dim=768,\n",
    "        depth=12,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.,\n",
    "        qkv_bias=True,\n",
    "        drop_rate=0.,\n",
    "        attn_drop_rate=0.,\n",
    "        reduction_ratio=2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.reduction_ratio = reduction_ratio\n",
    "        \n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_channels=in_channels,\n",
    "            embed_dim=embed_dim\n",
    "        )\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.patch_embed.n_patches + 1, embed_dim))\n",
    "        \n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            LinearTransformerBlock(\n",
    "                dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate,\n",
    "                reduction_ratio=reduction_ratio\n",
    "            )\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        \n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        x = self.patch_embed(x) \n",
    "        \n",
    "        cls_token = self.cls_token.expand(B, -1, -1)  \n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        \n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        cls_token_final = x[:, 0] \n",
    "        x = self.head(cls_token_final)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_layer_specs(self):\n",
    "        \"\"\"Returns a detailed summary of each layer's specifications\"\"\"\n",
    "        specs = OrderedDict()\n",
    "        \n",
    "        specs[\"Input\"] = {\n",
    "            \"shape\": f\"(batch_size, {self.in_channels}, {self.img_size}, {self.img_size})\",\n",
    "            \"params\": 0,\n",
    "            \"details\": \"Input image\"\n",
    "        }\n",
    "        \n",
    "        patch_params = sum(p.numel() for p in self.patch_embed.parameters())\n",
    "        specs[\"PatchEmbedding\"] = {\n",
    "            \"shape\": f\"(batch_size, {self.patch_embed.n_patches}, {self.embed_dim})\",\n",
    "            \"params\": patch_params,\n",
    "            \"details\": self.patch_embed.extra_repr()\n",
    "        }\n",
    "        \n",
    "        pos_params = self.pos_embed.numel() + self.cls_token.numel()\n",
    "        specs[\"PositionEmbedding\"] = {\n",
    "            \"shape\": f\"(1, {self.patch_embed.n_patches + 1}, {self.embed_dim})\",\n",
    "            \"params\": pos_params,\n",
    "            \"details\": \"Learned positional embeddings + class token\"\n",
    "        }\n",
    "        \n",
    "        specs[\"PosDrop\"] = {\n",
    "            \"shape\": f\"(batch_size, {self.patch_embed.n_patches + 1}, {self.embed_dim})\",\n",
    "            \"params\": 0,\n",
    "            \"details\": f\"Dropout(p={self.pos_drop.p})\"\n",
    "        }\n",
    "        \n",
    "        for i, block in enumerate(self.blocks):\n",
    "            block_params = sum(p.numel() for p in block.parameters())\n",
    "            specs[f\"LinearTransformerBlock_{i+1}\"] = {\n",
    "                \"shape\": f\"(batch_size, {self.patch_embed.n_patches + 1}, {self.embed_dim})\",\n",
    "                \"params\": block_params,\n",
    "                \"details\": block.extra_repr()\n",
    "            }\n",
    "        \n",
    "        norm_params = sum(p.numel() for p in self.norm.parameters())\n",
    "        specs[\"LayerNorm\"] = {\n",
    "            \"shape\": f\"(batch_size, {self.patch_embed.n_patches + 1}, {self.embed_dim})\",\n",
    "            \"params\": norm_params,\n",
    "            \"details\": f\"LayerNorm({self.embed_dim})\"\n",
    "        }\n",
    "        \n",
    "        # Classification head\n",
    "        head_params = sum(p.numel() for p in self.head.parameters()) if self.num_classes > 0 else 0\n",
    "        specs[\"Head\"] = {\n",
    "            \"shape\": f\"(batch_size, {self.num_classes})\",\n",
    "            \"params\": head_params,\n",
    "            \"details\": f\"Linear({self.embed_dim}->{self.num_classes})\" if self.num_classes > 0 else \"Identity\"\n",
    "        }\n",
    "        \n",
    "        return specs\n",
    "\n",
    "def print_model_summary(model):\n",
    "    \"\"\"Prints a detailed summary of the model architecture\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'Linear Vision Transformer (ViT) Summary':^80}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Image size: {model.img_size}x{model.img_size}\")\n",
    "    print(f\"Patch size: {model.patch_size}x{model.patch_size}\")\n",
    "    print(f\"Input channels: {model.in_channels}\")\n",
    "    print(f\"Embedding dimension: {model.embed_dim}\")\n",
    "    print(f\"Number of heads: {model.num_heads}\")\n",
    "    print(f\"Number of layers: {len(model.blocks)}\")\n",
    "    print(f\"MLP ratio: {model.blocks[0].mlp.fc1.out_features/model.embed_dim:.1f}\")\n",
    "    print(f\"Attention reduction ratio: {model.reduction_ratio}\")\n",
    "    print(f\"Number of classes: {model.num_classes}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nLayer Details:\\n\")\n",
    "    \n",
    "    specs = model.get_layer_specs()\n",
    "    max_layer_len = max(len(name) for name in specs.keys())\n",
    "    max_shape_len = max(len(spec[\"shape\"]) for spec in specs.values())\n",
    "    max_params_len = max(len(f\"{spec['params']:,}\") if isinstance(spec[\"params\"], int) \n",
    "                        else len(spec[\"params\"]) for spec in specs.values())\n",
    "    \n",
    "    total_params = 0\n",
    "    \n",
    "    for name, spec in specs.items():\n",
    "        params = spec[\"params\"]\n",
    "        if isinstance(params, int):\n",
    "            total_params += params\n",
    "            params_str = f\"{params:,}\"\n",
    "        else:\n",
    "            params_str = params\n",
    "            \n",
    "        print(f\"{name:<{max_layer_len}} | Shape: {spec['shape']:<{max_shape_len}} | \"\n",
    "              f\"Params: {params_str:<{max_params_len}} | Details: {spec['details']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"{'Total Parameters':<{max_layer_len}} : {total_params:,}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "        \"img_size\": 224,\n",
    "        \"patch_size\": 16,\n",
    "        \"in_channels\": 3,\n",
    "        \"num_classes\": 2,\n",
    "        \"embed_dim\": 768,\n",
    "        \"depth\": 8,\n",
    "        \"num_heads\": 8,\n",
    "        \"mlp_ratio\": 4.0,\n",
    "        \"qkv_bias\": True,\n",
    "        \"drop_rate\": 0.1,\n",
    "        \"attn_drop_rate\": 0.1,\n",
    "        \"reduction_ratio\": 4\n",
    "    }\n",
    "    \n",
    "    model = LinearViT(**config)\n",
    "    \n",
    "    print_model_summary(model)\n",
    "    \n",
    "    print(\"\\nStandard Torch Summary:\")\n",
    "    summary(model, (3, 224, 224), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c71f9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearViT(\n",
      "  43.13 M, 99.649% Params, 8.5 GMac, 99.247% MACs, \n",
      "  (patch_embed): PatchEmbedding(\n",
      "    590.59 k, 1.365% Params, 115.76 MMac, 1.352% MACs, img_size=224, patch_size=16, in_channels=3, embed_dim=768\n",
      "    (proj): Conv2d(590.59 k, 1.365% Params, 115.76 MMac, 1.352% MACs, 3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "  )\n",
      "  (pos_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    (0-7): 8 x LinearTransformerBlock(\n",
      "      5.32 M, 12.285% Params, 1.05 GMac, 12.237% MACs, dim=768, num_heads=8, mlp_ratio=4.0, reduction_ratio=4\n",
      "      (norm1): LayerNorm(1.54 k, 0.004% Params, 151.3 KMac, 0.002% MACs, (768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): ReducedLinearAttention(\n",
      "        591.17 k, 1.366% Params, 116.46 MMac, 1.360% MACs, dim=768, num_heads=8, reduced_dim=192, head_dim=24, reduction_ratio=4\n",
      "        (q): Linear(147.65 k, 0.341% Params, 29.09 MMac, 0.340% MACs, in_features=768, out_features=192, bias=True)\n",
      "        (kv): Linear(295.3 k, 0.682% Params, 58.17 MMac, 0.679% MACs, in_features=768, out_features=384, bias=True)\n",
      "        (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "        (proj): Linear(148.22 k, 0.342% Params, 29.2 MMac, 0.341% MACs, in_features=192, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(1.54 k, 0.004% Params, 151.3 KMac, 0.002% MACs, (768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        4.72 M, 10.912% Params, 930.92 MMac, 10.873% MACs, in_features=768, hidden_features=3072, out_features=768\n",
      "        (fc1): Linear(2.36 M, 5.458% Params, 465.39 MMac, 5.436% MACs, in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(0, 0.000% Params, 605.18 KMac, 0.007% MACs, approximate='none')\n",
      "        (fc2): Linear(2.36 M, 5.453% Params, 464.93 MMac, 5.430% MACs, in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm(1.54 k, 0.004% Params, 151.3 KMac, 0.002% MACs, (768,), eps=1e-05, elementwise_affine=True)\n",
      "  (head): Linear(1.54 k, 0.004% Params, 1.54 KMac, 0.000% MACs, in_features=768, out_features=2, bias=True)\n",
      ")\n",
      "FLOPs: 8.56 GMac, Parameters: 43.28 M\n"
     ]
    }
   ],
   "source": [
    "from ptflops import get_model_complexity_info\n",
    "macs, params = get_model_complexity_info(model, (3, 224, 224), as_strings=True)\n",
    "print(f\"FLOPs: {macs}, Parameters: {params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5221d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (tensorflow-gpu)",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
